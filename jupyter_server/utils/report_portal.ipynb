{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "FLASK_ADDR_DEFAULT = \"perf-insight.lab.eng.pek2.redhat.com:5000\"\n",
    "ENV_YAML = \"/root/.perf-insight.yaml\"\n",
    "\n",
    "WORKSPACE = \".\"\n",
    "BASE_TESTRUN_YAML = WORKSPACE + \"/base.generate_testrun_results.yaml\"\n",
    "TEST_TESTRUN_YAML = WORKSPACE + \"/test.generate_testrun_results.yaml\"\n",
    "METADATA_YAML = WORKSPACE + \"/generate_2way_metadata.yaml\"\n",
    "BENCHMARK_YAML = WORKSPACE + \"/generate_2way_benchmark.yaml\"\n",
    "\n",
    "BASE_DATASTORE = WORKSPACE + \"/base.datastore.json\"\n",
    "TEST_DATASTORE = WORKSPACE + \"/test.datastore.json\"\n",
    "BASE_METADATA = WORKSPACE + \"/base.metadata.json\"\n",
    "TEST_METADATA = WORKSPACE + \"/test.metadata.json\"\n",
    "\n",
    "BASE_TESTRUN_RESULT = WORKSPACE + \"/base.testrun_result.csv\"\n",
    "TEST_TESTRUN_RESULT = WORKSPACE + \"/test.testrun_result.csv\"\n",
    "METADATA = WORKSPACE + \"/2way_metadata.csv\"\n",
    "BENCHMARK = WORKSPACE + \"/2way_benchmark.csv\"\n",
    "PARAMETERS = WORKSPACE + \"/2way_parameters.csv\"\n",
    "STATISTICS = WORKSPACE + \"/2way_statistics.json\"\n",
    "SUMMARY = WORKSPACE + \"/2way_summary.csv\"\n",
    "\n",
    "SCRIPTPATH = WORKSPACE + \"/utils\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, Markdown\n",
    "from datetime import datetime\n",
    "\n",
    "BASEPATH = os.path.abspath('.')\n",
    "\n",
    "try:\n",
    "    with open(ENV_YAML) as f:\n",
    "        FLASK_ADDR = \"http://\" + yaml.safe_load(f).get(\"jupyter\").get(\"flask_server\", FLASK_ADDR_DEFAULT)\n",
    "except:\n",
    "    print(\"Cannot get Flask address. Use default value http://\" + FLASK_ADDR_DEFAULT)\n",
    "    FLASK_ADDR = \"http://\" + FLASK_ADDR_DEFAULT"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Perf-insight - Benchmark Report"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "now = datetime.now()\n",
    "dt_string = \"*Generate time: {}*\".format(now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "display(Markdown(dt_string))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Common functions\n",
    "def bold_font(s):\n",
    "    return 'font-weight: bold'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Generate Base testrun result script\n",
    "base_gen_result_script = \"{}/generate_testrun_results.py --config {} --datastore {} --metadata {} --output {}\".format(\n",
    "SCRIPTPATH, BASE_TESTRUN_YAML, BASE_DATASTORE, BASE_METADATA, BASE_TESTRUN_RESULT)\n",
    "\n",
    "# Generate Test testrun result script\n",
    "test_gen_result_script = \"{}/generate_testrun_results.py --config {} --datastore {} --metadata {} --output {}\".format(\n",
    "SCRIPTPATH, TEST_TESTRUN_YAML, TEST_DATASTORE, TEST_METADATA, TEST_TESTRUN_RESULT)\n",
    "\n",
    "# Generate 2way metadata script\n",
    "gen_metadata_script = \"{}/generate_2way_metadata.py --config {} --test {} --base {} --output {}\".format(\n",
    "SCRIPTPATH, METADATA_YAML, TEST_METADATA, BASE_METADATA, METADATA)\n",
    "\n",
    "# Generate 2way benchmark script\n",
    "gen_benchmark_script = \"{}/generate_2way_benchmark.py --config {} --test {} --base {} --output {}\".format(\n",
    "SCRIPTPATH, BENCHMARK_YAML, TEST_TESTRUN_RESULT, BASE_TESTRUN_RESULT, BENCHMARK)\n",
    "\n",
    "# Generate 2way parameters script\n",
    "gen_parameters_script = \"{}/generate_2way_parameters.py --benchmark-config {} --output {}\".format(\n",
    "SCRIPTPATH, BENCHMARK_YAML, PARAMETERS)\n",
    "\n",
    "# Generate 2way statistics script\n",
    "gen_statistics_script = \"{}/generate_2way_statistics.py --benchmark-csv {} --output {}\".format(\n",
    "SCRIPTPATH, BENCHMARK, STATISTICS)\n",
    "\n",
    "# Generate 2way summary script\n",
    "gen_summary_script = \"{}/generate_2way_summary.py --statistics-json {} --output {}\".format(\n",
    "SCRIPTPATH, STATISTICS, SUMMARY)\n",
    "\n",
    "\n",
    "# Run scripts parallelly\n",
    "import multiprocessing\n",
    "all_processes = (base_gen_result_script, test_gen_result_script, gen_metadata_script, gen_parameters_script)   \n",
    "\n",
    "def execute(process):                                                             \n",
    "    os.system(f'python3 {process}') \n",
    "\n",
    "process_pool = multiprocessing.Pool(processes = 4)                                                        \n",
    "process_pool.map(execute, all_processes)\n",
    "\n",
    "\n",
    "for result in [BASE_TESTRUN_RESULT, TEST_TESTRUN_RESULT, METADATA]:\n",
    "    assert os.path.exists(result), \"Fail to generate {}! Exit.\".format(result)\n",
    "\n",
    "# Generate 2way benchmark\n",
    "os.system('python3 {}'.format(gen_benchmark_script))\n",
    "assert os.path.exists(BENCHMARK), \"Fail to generate {}! Exit.\".format(BENCHMARK)\n",
    "\n",
    "# Generate 2way statistics and summary\n",
    "os.system('python3 {}'.format(gen_statistics_script))\n",
    "assert os.path.exists(STATISTICS), \"Fail to generate {}! Exit.\".format(STATISTICS)\n",
    "os.system('python3 {}'.format(gen_summary_script))\n",
    "assert os.path.exists(SUMMARY), \"Fail to generate {}! Exit.\".format(SUMMARY)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Format Summary\n",
    "def summary_color_delta(val):\n",
    "    with open(BENCHMARK_YAML) as f:\n",
    "        fillna = yaml.safe_load(f).get('benchmark_comparison_generator').get('defaults').get('fillna', \"NaN\")\n",
    "    color_dict = {\n",
    "        \"FAIL\": 'color: red; background-color: #FFB6C1',\n",
    "        \"PASS\": 'color: green; background-color: #F0FFF0',\n",
    "        fillna: 'color: #D3D3D3',\n",
    "    }\n",
    "    if '+' in val:\n",
    "        color = 'color: green'\n",
    "    elif '-' in val:\n",
    "        color = 'color: red'\n",
    "    else:\n",
    "        color = color_dict.get(val, 'color: black')\n",
    "    return color + '; font-weight: bold'\n",
    "\n",
    "\n",
    "# Hide header and enlarge font-size\n",
    "styles = [\n",
    "    dict(selector=\"th\", props=[(\"font-size\", \"0%\")]),\n",
    "    dict(selector=\"\", props=[(\"font-size\", \"102%\")])\n",
    "]\n",
    "\n",
    "if os.path.exists(SUMMARY):\n",
    "    # Read summary and display\n",
    "    display(Markdown(\"## Summary\"))\n",
    "    summary_df = pd.read_csv(SUMMARY, keep_default_na=False, index_col=0)\n",
    "    display(summary_df.style.set_table_styles(styles).applymap(summary_color_delta,subset=[\"VALUE\"]).hide_index())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def read_json(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(\"Fail to load {}\".format(json_file))\n",
    "            raise\n",
    "    return data\n",
    "\n",
    "base_metadata = read_json(BASE_METADATA)\n",
    "test_metadata = read_json(TEST_METADATA)\n",
    "assert base_metadata.get(\"testrun-type\") == test_metadata.get(\"testrun-type\"), \"Base and Test type must be the same! Exit.\"\n",
    "\n",
    "run_type = base_metadata.get(\"testrun-type\")\n",
    "base_platform = base_metadata.get(\"testrun-platform\")\n",
    "test_platform = test_metadata.get(\"testrun-platform\")\n",
    "base_id = base_metadata.get(\"testrun-id\")\n",
    "test_id = test_metadata.get(\"testrun-id\")\n",
    "# Type and platform must not be None\n",
    "assert run_type is not None, \"Type is None! Exit.\"\n",
    "assert base_platform is not None, \"Base platform is None! Exit.\"\n",
    "assert test_platform is not None, \"Test platform is None! Exit.\"\n",
    "\n",
    "#with open('{}/templates/introduction_{}_{}.md'.format(BASEPATH, base_platform.lower(), run_type), 'r') as f:\n",
    "#    display(Markdown(f.read()))\n",
    "\n",
    "with open('{}/testrun_introduction.md'.format(BASEPATH, base_platform.lower(), run_type), 'r') as f:\n",
    "    display(Markdown(f.read()))\n",
    "    \n",
    "    \n",
    "if base_platform != test_platform:\n",
    "    with open('{}/introduction_{}_{}.md'.format(BASEPATH, test_platform.lower(), run_type), 'r') as f:\n",
    "        display(Markdown('\\n'+f.read()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%html\n",
    "<style>\n",
    "    table {\n",
    "        display: inline-block\n",
    "    }\n",
    "</style>"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metadata"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%HTML\n",
    "* The differences between Test and Base are <b style='color:orange'>highlighted</b>."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Format metadata\n",
    "def highlight_diff(row, cell_format):\n",
    "    cell_format = cell_format if row['TEST'] != row['BASE'] else ''\n",
    "    format_row = ['']*(len(row)-2) + [cell_format, cell_format]\n",
    "    return format_row\n",
    "\n",
    "def color_diff(row):\n",
    "    return highlight_diff(row, 'color: orange')\n",
    "\n",
    "def bold_diff(row):\n",
    "    return highlight_diff(row, 'font-weight: bold')\n",
    "    \n",
    "conf_df = pd.read_csv(METADATA, index_col=0, keep_default_na=False)\n",
    "#conf_df = conf_df[['NAME', 'TEST', 'BASE']]\n",
    "display(conf_df.style.applymap(bold_font, subset=['NAME']).apply(color_diff, axis=1).apply(bold_diff, axis=1).hide_index())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Benchmark Introduction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Show benchmark report description\n",
    "with open('{}/benchmark_description.md'.format(BASEPATH), 'r') as f:\n",
    "    display(Markdown(f.read()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Benchmark Report"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**User parameters**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Show user parameters\n",
    "with open(PARAMETERS) as f:\n",
    "    param_df = pd.read_csv(f, index_col=0, dtype=str, keep_default_na=False)\n",
    "    \n",
    "display(param_df.style.applymap(bold_font, subset=['name']).hide_index())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Detailed Report**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Format benchmark report\n",
    "def benchmark_color_delta(val):\n",
    "    with open(BENCHMARK_YAML) as f:\n",
    "        fillna = yaml.safe_load(f).get('benchmark_comparison_generator').get(\n",
    "            'defaults').get('fillna', \"NaN\")\n",
    "    color_dict = {\n",
    "        \"Dramatic Regression\": 'color: red; background-color: #FFB6C1',\n",
    "        \"Moderate Regression\": 'color: red',\n",
    "        \"Dramatic Improvement\": 'color: green; background-color: #F0FFF0',\n",
    "        \"Moderate Improvement\": 'color: green',\n",
    "        \"High Variance\": 'color: orange; background-color: #FAFAD2',\n",
    "        \"No Significance\": 'color: gray',\n",
    "        \"Negligible Changes\": 'color: gray',\n",
    "        \"Invalid Data\": 'color: red',\n",
    "        \"DR\": 'color: red; background-color: #FFB6C1',\n",
    "        \"MR\": 'color: red',\n",
    "        \"DI\": 'color: green; background-color: #F0FFF0',\n",
    "        \"MI\": 'color: green',\n",
    "        \"HV\": 'color: orange; background-color: #FAFAD2',\n",
    "        \"NS\": 'color: gray',\n",
    "        \"NC\": 'color: gray',\n",
    "        \"ID\": 'color: red',\n",
    "        fillna: 'color: #D3D3D3'\n",
    "    }\n",
    "    return '{}'.format(color_dict.get(val, 'color: black'))\n",
    "\n",
    "\n",
    "def displayComparison(df):\n",
    "    # These are the columns which need special formatting\n",
    "    deltacols = df.columns.map(lambda x: x.endswith((\"-CON\", \"Conclusion\")))\n",
    "    display(df.style\n",
    "            .applymap(benchmark_color_delta, subset=deltacols)\n",
    "            .applymap(bold_font, subset=deltacols)\n",
    "            .format({'Test': lambda x: '<a target=\"_blank\" href=\"{}\">link</a>'.format(x), 'Base': lambda x: '<a target=\"_blank\" href=\"{}\">link</a>'.format(x)}))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_fio_report_link(row, metadata):\n",
    "#     info = {\n",
    "#         \"flask_addr\": FLASK_ADDR,\n",
    "#         \"testrun\": metadata.get('testrun-id'),\n",
    "#         \"platform\": metadata.get('testrun-platform'),\n",
    "#         \"backend\": metadata.get('hardware-disk-backend', ''),\n",
    "#         'driver': metadata.get('hardware-disk-driver', ''),\n",
    "#         'format': metadata.get('hardware-disk-format', ''),\n",
    "# #        'iodepth': row['IOdepth'],\n",
    "# #        'numjobs': row['Numjobs'],\n",
    "# #        'bs': row['BS'],\n",
    "# #        'rw': row['RW'],\n",
    "#         'case_id': row['CaseID'],\n",
    "#             }\n",
    "#     url = \"{}/storageresultpubview/list/?\".format(FLASK_ADDR)\n",
    "#     return \"{flask_addr}/storageresultpubview/list/?\\\n",
    "# _flt_3_testrun={testrun_id}&\\\n",
    "# _flt_3_platform={platform}&\\\n",
    "# _flt_3_backend={backend}&\\\n",
    "# _flt_3_driver={driver}&\\\n",
    "# _flt_3_format={format}&\\\n",
    "# _flt_3_case_id={case_id}\\\n",
    "# \".format(**info)\n",
    "\n",
    "    info = {\n",
    "        \"testrun\": metadata.get('testrun-id'),\n",
    "        \"platform\": metadata.get('testrun-platform'),\n",
    "        \"backend\": metadata.get('hardware-disk-backend'),\n",
    "        'driver': metadata.get('hardware-disk-driver'),\n",
    "        'format': metadata.get('hardware-disk-format'),\n",
    "        'case_id': row['CaseID'],\n",
    "    }\n",
    "\n",
    "    url = \"{}/storageresultpubview/list/?\".format(FLASK_ADDR)\n",
    "    for key, value in info.items():\n",
    "        if value is not None:\n",
    "            url += '_flt_3_' + key + '=' + value + '&'\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_uperf_report_link(row, metadata):\n",
    "#     info = {\n",
    "#         \"flask_addr\": FLASK_ADDR,\n",
    "#         \"testrun_id\": metadata.get('testrun-id'),\n",
    "#         \"platform\": metadata.get('testrun-platform'),\n",
    "#         ''\n",
    "#         'protocol': row['Protocol'],\n",
    "#         'testtype': row['TestType'],\n",
    "#         'msize': row['MSize'],\n",
    "#         'instance': row['Instance']\n",
    "#     }\n",
    "#     return \"{flask_addr}/networkresultpubview/list/?\\\n",
    "# _flt_0_testrun={testrun_id}&\\\n",
    "# _flt_0_platform={platform}&\\\n",
    "# _flt_0_protocol={protocol}&\\\n",
    "# _flt_0_testtype={testtype}&\\\n",
    "# _flt_0_msize={msize}&\\\n",
    "# _flt_0_instance={instance}\\\n",
    "# \".format(**info)\n",
    "\n",
    "    ## flt_3 is \"equal to\", flt_0 is \"starts with\".\n",
    "\n",
    "    info = {\n",
    "        \"testrun_id\": metadata.get('testrun-id'),\n",
    "        \"platform\": metadata.get('testrun-platform'),\n",
    "        'case_id': row['CaseID'],\n",
    "    }\n",
    "\n",
    "    url = \"{}/networkresultpubview/list/?\".format(FLASK_ADDR)\n",
    "    for key, value in info.items():\n",
    "        if value is not None:\n",
    "            url += '_flt_3_' + key + '=' + value + '&'\n",
    "    return url\n",
    "\n",
    "def get_report_link(row, metadata_file):\n",
    "    with open(metadata_file) as f:\n",
    "        metadata = json.load(f)\n",
    "    testrun_type = metadata.get('testrun-type')\n",
    "    if testrun_type == \"uperf\":\n",
    "        return get_uperf_report_link(row, metadata)\n",
    "    elif testrun_type == \"fio\":\n",
    "        return get_fio_report_link(row, metadata)\n",
    "    else:\n",
    "        return FLASK_ADDR\n",
    "\n",
    "\n",
    "benchmark_df = pd.read_csv(BENCHMARK, index_col=0, dtype=str, keep_default_na=False)\n",
    "#summary_df = benchmark_df[['RW','BS','IOdepth','Numjobs']+list(benchmark_df.filter(regex='-CON$').columns)]\n",
    "benchmark_df.insert(0, 'Test', benchmark_df.apply(lambda row: get_report_link(row, TEST_METADATA), axis=1))\n",
    "benchmark_df.insert(0, 'Base', benchmark_df.apply(lambda row: get_report_link(row, BASE_METADATA), axis=1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import ipywidgets as widgets\n",
    "import functools\n",
    "\n",
    "# Get our unique values\n",
    "ALL = 'ALL'\n",
    "RESULT_COLUMN = \"Conclusion\"\n",
    "\n",
    "def unique_sorted_values_plus_ALL(array):\n",
    "    unique = array.unique().tolist()\n",
    "    unique.sort()\n",
    "    unique.insert(0, ALL)\n",
    "    return unique\n",
    "\n",
    "filtered = pd.DataFrame()\n",
    "output = widgets.Output()\n",
    "\n",
    "# TBD: add more dropdown items\n",
    "dropdown_list = []\n",
    "# Result column dropdown widget\n",
    "dropdown_list.append(widgets.Dropdown(\n",
    "                                options = unique_sorted_values_plus_ALL(benchmark_df[RESULT_COLUMN]),\n",
    "                                layout={'height':'30px', 'width':'20%'}))\n",
    "\n",
    "# Details checkbox widget to show/hide -AVG, -%SD and SGN columns\n",
    "detail_checkbox = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Show Details',\n",
    "    disabled=False,\n",
    "    indent=True\n",
    ")\n",
    "\n",
    "# The detail checkbox description label is long, so need to customize label layout\n",
    "detail_label = widgets.Label('Conclusion Filter:', layout=widgets.Layout(width='120px',height='30px'))\n",
    "\n",
    "widget_list = [detail_label] + dropdown_list + [detail_checkbox]\n",
    "\n",
    "def detail_filtering(detail, df):\n",
    "    if not detail:\n",
    "        show_cols = [col for col in df.columns.tolist() if ('-AVG' not in col and '-%SD' not in col and '-SGN' not in col)]\n",
    "        return df[show_cols]\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "def common_filtering(cols, detail):\n",
    "    \"\"\"\n",
    "    cols: dict, {'column_name': dropdown widget value}\n",
    "    details: bool, checkbox widget value\n",
    "    \"\"\"\n",
    "    \n",
    "    global filtered\n",
    "    output.clear_output()\n",
    "\n",
    "    filters= []\n",
    "    for name, value in cols.items():\n",
    "        if value != ALL:\n",
    "            filters.append(benchmark_df[name] == value)\n",
    "\n",
    "    if filters:\n",
    "        df_filter = functools.reduce(lambda x,y: x&y, filters)\n",
    "        filtered = benchmark_df[df_filter]\n",
    "    else:\n",
    "        filtered = benchmark_df\n",
    "        \n",
    "    filtered = detail_filtering(detail, filtered)\n",
    "\n",
    "    with output:\n",
    "        displayComparison(filtered)\n",
    "    \n",
    "def result_dropdown_eventhandler(change):\n",
    "    common_filtering({RESULT_COLUMN: change.new}, detail_checkbox.value)\n",
    "    \n",
    "def detail_checkbox_eventhandler(change):\n",
    "    common_filtering({RESULT_COLUMN: dropdown_list[0].value}, change.new)\n",
    "\n",
    "dropdown_list[0].observe(result_dropdown_eventhandler, names='value')\n",
    "detail_checkbox.observe(detail_checkbox_eventhandler, names='value')\n",
    "\n",
    "input_widgets = widgets.HBox(widget_list)\n",
    "display(input_widgets)\n",
    "\n",
    "# To workaround no horizon scrollbar issue\n",
    "output.layout.width = '10000px'\n",
    "\n",
    "with output:\n",
    "    displayComparison(detail_filtering(False, benchmark_df))\n",
    "display(output)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
