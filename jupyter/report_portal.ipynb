{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLASK_ADDR_DEFAULT = \"perf-insight.lab.eng.pek2.redhat.com:5000\"\n",
    "ENV_YAML = \"/root/.perf-insight.yaml\"\n",
    "\n",
    "WORKSPACE = \"/workspace\"\n",
    "YAML = WORKSPACE + \"/benchmark_config.yaml\"\n",
    "BASE_DATASTORE = WORKSPACE + \"/base.datastore.json\"\n",
    "TEST_DATASTORE = WORKSPACE + \"/test.datastore.json\"\n",
    "BASE_METADATA = WORKSPACE + \"/base.metadata.json\"\n",
    "TEST_METADATA = WORKSPACE + \"/test.metadata.json\"\n",
    "\n",
    "BASE_TESTRUN_RESULT = WORKSPACE + \"/base.testrun_result.csv\"\n",
    "TEST_TESTRUN_RESULT = WORKSPACE + \"/test.testrun_result.csv\"\n",
    "METADATA = WORKSPACE + \"/2way_metadata.csv\"\n",
    "BENCHMARK = WORKSPACE + \"/2way_benchmark.csv\"\n",
    "PARAMETERS = WORKSPACE + \"/2way_parameters.csv\"\n",
    "STATISTICS = WORKSPACE + \"/2way_statistics.json\"\n",
    "SUMMARY = WORKSPACE + \"/2way_summary.csv\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, Markdown\n",
    "from datetime import datetime\n",
    "\n",
    "BASEPATH = os.path.abspath('.')\n",
    "SCRIPTPATH = BASEPATH + \"/../data_process\"\n",
    "\n",
    "try:\n",
    "    with open(ENV_YAML) as f:\n",
    "        FLASK_ADDR = \"http://\" + yaml.safe_load(f).get(\"jupyter\").get(\"flask_server\", FLASK_ADDR_DEFAULT)\n",
    "except:\n",
    "    print(\"Cannot get Flask address. Use default value http://\" + FLASK_ADDR_DEFAULT)\n",
    "    FLASK_ADDR = \"http://\" + FLASK_ADDR_DEFAULT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perf-insight - 2way Benchmark Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "dt_string = \"*Generate time: {}*\".format(now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "display(Markdown(dt_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common functions\n",
    "def bold_font(s):\n",
    "    return 'font-weight: bold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Base testrun result script\n",
    "base_gen_result_script = \"{}/generate_testrun_results.py --config {} --datastore {} --metadata {} --output {}\".format(\n",
    "SCRIPTPATH, YAML, BASE_DATASTORE, BASE_METADATA, BASE_TESTRUN_RESULT)\n",
    "\n",
    "# Generate Test testrun result script\n",
    "test_gen_result_script = \"{}/generate_testrun_results.py --config {} --datastore {} --metadata {} --output {}\".format(\n",
    "SCRIPTPATH, YAML, TEST_DATASTORE, TEST_METADATA, TEST_TESTRUN_RESULT)\n",
    "\n",
    "# Generate 2way metadata script\n",
    "gen_metadata_script = \"{}/generate_2way_metadata.py --config {} --test {} --base {} --output {}\".format(\n",
    "SCRIPTPATH, YAML, TEST_METADATA, BASE_METADATA, METADATA)\n",
    "\n",
    "# Generate 2way benchmark script\n",
    "gen_benchmark_script = \"{}/generate_2way_benchmark.py --config {} --test {} --base {} --output {}\".format(\n",
    "SCRIPTPATH, YAML, TEST_TESTRUN_RESULT, BASE_TESTRUN_RESULT, BENCHMARK)\n",
    "\n",
    "# Generate 2way parameters script\n",
    "gen_parameters_script = \"{}/generate_2way_parameters.py --benchmark-config {} --output {}\".format(\n",
    "SCRIPTPATH, YAML, PARAMETERS)\n",
    "\n",
    "# Generate 2way statistics script\n",
    "gen_statistics_script = \"{}/generate_2way_statistics.py --benchmark-csv {} --output {}\".format(\n",
    "SCRIPTPATH, BENCHMARK, STATISTICS)\n",
    "\n",
    "# Generate 2way summary script\n",
    "gen_summary_script = \"{}/generate_2way_summary.py --statistics-json {} --output {}\".format(\n",
    "SCRIPTPATH, STATISTICS, SUMMARY)\n",
    "\n",
    "# Run scripts parallelly\n",
    "import multiprocessing\n",
    "all_processes = (base_gen_result_script, test_gen_result_script, gen_metadata_script, gen_parameters_script)   \n",
    "\n",
    "def execute(process):                                                             \n",
    "    os.system(f'python3 {process}') \n",
    "\n",
    "process_pool = multiprocessing.Pool(processes = 4)                                                        \n",
    "process_pool.map(execute, all_processes)\n",
    "\n",
    "for result in [BASE_TESTRUN_RESULT, TEST_TESTRUN_RESULT, METADATA]:\n",
    "    assert os.path.exists(result), \"Failed to generate {}! Exit.\".format(result)\n",
    "\n",
    "# Generate 2way benchmark\n",
    "os.system('python3 {}'.format(gen_benchmark_script))\n",
    "assert os.path.exists(BENCHMARK), \"Failed to generate {}! Exit.\".format(BENCHMARK)\n",
    "\n",
    "# Generate 2way statistics and summary\n",
    "os.system('python3 {}'.format(gen_statistics_script))\n",
    "assert os.path.exists(STATISTICS), \"Failed to generate {}! Exit.\".format(STATISTICS)\n",
    "os.system('python3 {}'.format(gen_summary_script))\n",
    "assert os.path.exists(SUMMARY), \"Failed to generate {}! Exit.\".format(SUMMARY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Summary\n",
    "def summary_color_delta(val):\n",
    "    with open(YAML) as f:\n",
    "        fillna = yaml.safe_load(f).get('benchmark_comparison_generator').get('defaults').get('fillna', \"NaN\")\n",
    "    color_dict = {\n",
    "        \"FAIL\": 'color: red; background-color: #FFB6C1',\n",
    "        \"PASS\": 'color: green; background-color: #F0FFF0',\n",
    "        fillna: 'color: #D3D3D3',\n",
    "    }\n",
    "    if '+' in val:\n",
    "        color = 'color: green'\n",
    "    elif '-' in val:\n",
    "        color = 'color: red'\n",
    "    else:\n",
    "        color = color_dict.get(val, 'color: black')\n",
    "    return color + '; font-weight: bold'\n",
    "\n",
    "\n",
    "# Hide header and enlarge font-size\n",
    "styles = [\n",
    "    dict(selector=\"th\", props=[(\"font-size\", \"0%\")]),\n",
    "    dict(selector=\"\", props=[(\"font-size\", \"102%\")])\n",
    "]\n",
    "\n",
    "if os.path.exists(SUMMARY):\n",
    "    # Read summary and display\n",
    "    display(Markdown(\"## Summary\"))\n",
    "    summary_df = pd.read_csv(SUMMARY, keep_default_na=False, index_col=0)\n",
    "    display(summary_df.style.set_table_styles(styles).applymap(summary_color_delta,subset=[\"VALUE\"]).hide_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(\"Failed to load {}\".format(json_file))\n",
    "            raise\n",
    "    return data\n",
    "\n",
    "base_metadata = read_json(BASE_METADATA)\n",
    "test_metadata = read_json(TEST_METADATA)\n",
    "assert base_metadata.get(\"testrun-type\") == test_metadata.get(\"testrun-type\"), \"Base and Test type must be the same! Exit.\"\n",
    "\n",
    "run_type = base_metadata.get(\"testrun-type\")\n",
    "base_platform = base_metadata.get(\"testrun-platform\")\n",
    "test_platform = test_metadata.get(\"testrun-platform\")\n",
    "base_id = base_metadata.get(\"testrun-id\")\n",
    "test_id = test_metadata.get(\"testrun-id\")\n",
    "# Type and platform must not be None\n",
    "assert run_type is not None, \"Type is None! Exit.\"\n",
    "assert base_platform is not None, \"Base platform is None! Exit.\"\n",
    "assert test_platform is not None, \"Test platform is None! Exit.\"\n",
    "\n",
    "with open('{}/templates/introduction_{}_{}.md'.format(BASEPATH, base_platform.lower(), run_type), 'r') as f:\n",
    "    display(Markdown(f.read()))\n",
    "    \n",
    "if base_platform != test_platform:\n",
    "    with open('{}/templates/introduction_{}_{}.md'.format(BASEPATH, test_platform.lower(), run_type), 'r') as f:\n",
    "        display(Markdown('\\n'+f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "    table {\n",
    "        display: inline-block\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "* The differences between Test and Base are <b style='color:orange'>highlighted</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Format metadata\n",
    "def highlight_diff(row, cell_format):\n",
    "    cell_format = cell_format if row['TEST'] != row['BASE'] else ''\n",
    "    format_row = ['']*(len(row)-2) + [cell_format, cell_format]\n",
    "    return format_row\n",
    "\n",
    "def color_diff(row):\n",
    "    return highlight_diff(row, 'color: orange')\n",
    "\n",
    "def bold_diff(row):\n",
    "    return highlight_diff(row, 'font-weight: bold')\n",
    "    \n",
    "conf_df = pd.read_csv(METADATA, index_col=0, keep_default_na=False)\n",
    "#conf_df = conf_df[['NAME', 'TEST', 'BASE']]\n",
    "display(conf_df.style.applymap(bold_font, subset=['NAME']).apply(color_diff, axis=1).apply(bold_diff, axis=1).hide_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show benchmark report description\n",
    "with open('{}/templates/benchmark_description.md'.format(BASEPATH), 'r') as f:\n",
    "    display(Markdown(f.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show user parameters\n",
    "with open(PARAMETERS) as f:\n",
    "    param_df = pd.read_csv(f, index_col=0, dtype=str, keep_default_na=False)\n",
    "    \n",
    "display(param_df.style.applymap(bold_font, subset=['name']).hide_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detailed Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format benchmark report\n",
    "def benchmark_color_delta(val):\n",
    "    with open(YAML) as f:\n",
    "        fillna = yaml.safe_load(f).get('benchmark_comparison_generator').get('defaults').get('fillna', \"NaN\")\n",
    "    color_dict = {\n",
    "        \"DR\": 'color: red; background-color: #FFB6C1',\n",
    "        \"MR\": 'color: red',\n",
    "        \"DI\": 'color: green; background-color: #F0FFF0',\n",
    "        \"MI\": 'color: green',\n",
    "        \"HV\": 'color: orange; background-color: #FAFAD2',\n",
    "        \"NS\": 'color: gray',\n",
    "        \"NC\": 'color: gray',\n",
    "        \"ID\": 'color: red',\n",
    "        fillna: 'color: #D3D3D3',\n",
    "        \n",
    "    }\n",
    "    return '{}'.format(color_dict.get(val, 'color: black'))\n",
    "\n",
    "def displayComparison(df):\n",
    "    #These are the columns which need special formatting\n",
    "    deltacols=df.columns.map(lambda x: x.endswith(\"-CON\"))\n",
    "    display(df.style\\\n",
    "            .applymap(benchmark_color_delta,subset=deltacols)\\\n",
    "            .applymap(bold_font,subset=deltacols)\\\n",
    "            .format({'Test': lambda x: '<a target=\"_blank\" href=\"{}\">link</a>'.format(x)})\\\n",
    "            .format({'Base': lambda x: '<a target=\"_blank\" href=\"{}\">link</a>'.format(x)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_fio_report_link(row, metadata):\n",
    "    info = {\n",
    "        \"flask_addr\": FLASK_ADDR,\n",
    "        \"testrun_id\": metadata.get('testrun-id'),\n",
    "        \"platform\": metadata.get('testrun-platform'),\n",
    "        \"backend\": metadata.get('hardware-disk-backend'),\n",
    "        'driver': metadata.get('hardware-disk-driver'),\n",
    "        'format': metadata.get('hardware-disk-format'),\n",
    "        'iodepth': row['IOdepth'],\n",
    "        'numjobs': row['Numjobs'],\n",
    "        'bs': row['BS'],\n",
    "        'rw': row['RW'],\n",
    "            }\n",
    "    return \"{flask_addr}/storageresultpubview/list/?\\\n",
    "_flt_3_testrun={testrun_id}&\\\n",
    "_flt_3_platform={platform}&\\\n",
    "_flt_3_backend={backend}&\\\n",
    "_flt_3_driver={driver}&\\\n",
    "_flt_3_format={format}&\\\n",
    "_flt_3_bs={bs}&\\\n",
    "_flt_3_rw={rw}&\\\n",
    "_flt_0_iodepth={iodepth}&\\\n",
    "_flt_0_numjobs={numjobs}\\\n",
    "\".format(**info)\n",
    "\n",
    "\n",
    "def get_uperf_report_link(row, metadata):\n",
    "    info = {\n",
    "        \"flask_addr\": FLASK_ADDR,\n",
    "        \"testrun_id\": metadata.get('testrun-id'),\n",
    "        \"platform\": metadata.get('testrun-platform'),\n",
    "        ''\n",
    "        'protocol': row['Protocol'],\n",
    "        'testtype': row['TestType'],\n",
    "        'msize': row['MSize'],\n",
    "        'instance': row['Instance']\n",
    "    }\n",
    "    return \"{flask_addr}/networkresultpubview/list/?\\\n",
    "_flt_0_testrun={testrun_id}&\\\n",
    "_flt_0_platform={platform}&\\\n",
    "_flt_0_protocol={protocol}&\\\n",
    "_flt_0_testtype={testtype}&\\\n",
    "_flt_0_msize={msize}&\\\n",
    "_flt_0_instance={instance}\\\n",
    "\".format(**info)\n",
    "\n",
    "\n",
    "def get_report_link(row, metadata_file):\n",
    "    with open(metadata_file) as f:\n",
    "        metadata = json.load(f)\n",
    "    testrun_type = metadata.get('testrun-type')\n",
    "    if testrun_type == \"uperf\":\n",
    "        return get_uperf_report_link(row, metadata)\n",
    "    elif testrun_type == \"fio\":\n",
    "        return get_fio_report_link(row, metadata)\n",
    "    else:\n",
    "        return FLASK_ADDR\n",
    "\n",
    "\n",
    "benchmark_df = pd.read_csv(BENCHMARK, index_col=0, dtype=str, keep_default_na=False)\n",
    "#summary_df = benchmark_df[['RW','BS','IOdepth','Numjobs']+list(benchmark_df.filter(regex='-CON$').columns)]\n",
    "benchmark_df.insert(0, 'Test', benchmark_df.apply(lambda row: get_report_link(row, TEST_METADATA), axis=1))\n",
    "benchmark_df.insert(0, 'Base', benchmark_df.apply(lambda row: get_report_link(row, BASE_METADATA), axis=1))\n",
    "\n",
    "displayComparison(benchmark_df)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}