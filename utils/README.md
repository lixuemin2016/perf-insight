# Introduction

Some data processing scripts for the reporting system (perf-insight).  
They pick data from pbench-agent output, do further analysis, deal with additional metadata and finally convert them into reporting system wanted format. The scripts are designed to keep the current pbench data model.

# Usage

## Configuration

You need to have a configuration file under home directory named `.perf-insight.yaml`, and its content could be like:

```
flask:
  data_path: /nfs/perf-insight
  apache_server: 10.73.199.224
jupyter:
  flask_server: 10.73.199.224:5000
  apache_server: 10.73.199.224
```

The `flask` block provides parameters for the Flask component, the same way for the `jupyter` block.

## Before you start

In this project, most of the scripts take the `datastore.json` as an input. This json file is composed by collecting "iteration data" generated by the pbench-agent and injected with some additional information. So it keeps the same data model as the pbench project.

Run the following script to generate the `datastore.json`:

```bash
$ ll ./data_source/fio_ESXi_RHEL-8.3.0-2020111009.2_lite_scsi_D201203T114029
total 7.0M
drwxr-xr-x. 6 root root 4.0K Dec  9 16:38 fio_ESXi_RHEL-8.3.0-2020111009.2_lite_scsi_D201203T114029_2020.12.03T03.40.30
drwxr-xr-x. 6 root root 4.0K Dec  9 16:38 fio_ESXi_RHEL-8.3.0-2020111009.2_lite_scsi_D201203T114029_2020.12.03T03.52.20
drwxr-xr-x. 6 root root 4.0K Dec  9 16:38 fio_ESXi_RHEL-8.3.0-2020111009.2_lite_scsi_D201203T114029_2020.12.03T04.04.42
drwxr-xr-x. 6 root root 4.0K Dec  9 16:38 fio_ESXi_RHEL-8.3.0-2020111009.2_lite_scsi_D201203T114029_2020.12.03T04.15.34
-rw-r--r--. 1 root root  603 Dec  9 16:41 metadata.json

$ ./gather_testrun_datastore.py \
    --logdir ./data_source/fio_ESXi_RHEL-8.3.0-2020111009.2_lite_scsi_D201203T114029/ \
    --testrun fio_ESXi_RHEL-8.3.0-2020111009.2_lite_scsi_D201203T114029 \
    --output ./workspace/datastore.json

$ ll ./workspace 
total 6.9M
-rw-rw-r--. 1 cheshi cheshi 6.9M Dec 16 13:29 datastore.json
```

## Flask

### Preparation

Before you start, you need to prepare the following stuff:
1. datastore.json - contains the performance data of a testrun.
2. metadata.json - contains the metadata of a testrun.
3. generate_testrun_results.yaml - user config.

The following scripts are needed:
1. generate_testrun_results.py - generate a CSV file for the database to load.
2. load_db.py (TBD) - load the data into the Flask database.

### Steps

Run the following script to generate the CSV file:

```bash
$ ./generate_testrun_results.py \
    --config ./templates/generate_testrun_results-flask_fio.yaml \
    --datastore ./workspace/datastore.json \
    --metadata ./data_source/fio_ESXi_RHEL-8.3.0-2020111009.2_lite_scsi_D201203T114029/metadata.json \
    --output-format csv \
    --output ./workspace/testrun_results.csv
```

The `testrun_results.csv` will contain all the fields which the flask database needs.

Run the following script to load the data into database:

```bash
# To be done from Frank side.
```

## Jupyter

### Preparation

Before you start, you need to prepare the following stuff:
1. datastore.json - contains the performance data of a testrun.
2. metadata.json - contains the metadata of a testrun.
3. generate_testrun_results.yaml - user config for generating test summary.
4. generate_benchmark_results.yaml - user config for generating benchmark reports.

**Notes:** in a typical benchmark comparison, the `datastore.json` and `metadata.json` are needed for both TEST and BASE runs.

The following scripts are needed:
1. generate_testrun_results.py - generate the test summary for Jupyter to display.
2. generate_benchmark_metadata.py - generate the metadata comparison for Jupyter to display.
3. generate_benchmark_parameters.py - generate the benchmark parameters for Jupyter to display.
4. generate_benchmark_results.py - generate the benchmark results for Jupyter to display.

### Steps

```bash
$ ll ./workspace/
total 32M
-rw-rw-r--. 1 cheshi cheshi 6.9M Dec 16 13:29 base.datastore.json
-rw-r--r--. 1 cheshi cheshi  626 Dec 16 13:55 base.metadata.json
-rw-r--r--. 1 cheshi cheshi  25M Dec 16 13:56 test.datastore.json
-rw-r--r--. 1 cheshi cheshi  620 Dec 16 13:57 test.metadata.json
lrwxrwxrwx. 1 cheshi cheshi   31 Dec 16 14:33 generate_benchmark_results.yaml -> ../generate_benchmark_results.yaml
lrwxrwxrwx. 1 cheshi cheshi   54 Dec 16 14:32 generate_testrun_results.yaml -> ../templates/generate_testrun_results-jupyter_fio.yaml
```

Run the following script to generate the test summary:

```bash
$ ./generate_testrun_results.py \
    --config ./workspace/generate_testrun_results.yaml \
    --datastore ./workspace/test.datastore.json \
    --metadata ./workspace/test.metadata.json \
    --output-format csv \
    --output ./workspace/test.testrun_results.csv
```

Do it again for the BASE run.

Run the following script to generate the metadata comparison:

```bash
$ ./generate_benchmark_metadata.py \
    --test ./workspace/test.metadata.json \
    --base ./workspace/base.metadata.json \
    --output-format csv \
    --output ./workspace/benchmark_metadata.csv
```

Run the following script to generate the benchmark parameters:

```bash
$ ./generate_benchmark_parameters.py \
    --benchmark-config ./workspace/generate_benchmark_results.yaml \
    --output-format csv \
    --output ./workspace/benchmark_parameters.csv
```

Run the following script to generate the benchmark results:

```bash
$ ./generate_benchmark_results.py \
    --config ./workspace/generate_benchmark_results.yaml \
    --test ./workspace/test.testrun_results.csv \
    --base ./workspace/base.testrun_results.csv \
    --output-format csv \
    --output ./workspace/benchmark_results.csv
```
